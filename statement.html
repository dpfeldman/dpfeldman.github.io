
<html>
<head>
<title> Statement of Research Interests </title>
</head>

<center>
<h1> Statement of Research Interests </h1>
<h2> David P. Feldman </h2>
</center>


How does nature self-organize and how can scientists discover such
organization?  Is there an objective notion of pattern,
or is the discovery of patterns a purely subjective process?  And what
vocabulary is appropriate for describing and quantifying pattern and
organization?  In the course of my dissertation research I've found
that it is possible to give a precise, quantitative answer to these rather
abstract questions.  To do so, I use information theory and the
theory of formal computation to inquire how a system {\em
computes}: how it stores and manipulates historical information
to produce future behavior.  
<p>

My work to date has focused on statistical mechanical models of
magnetic phase transitions.  In these systems, ``pattern'' and
``structure'' are typically measured by correlation functions,
structure factors, and by experimental quantities such as the specific
heat and magnetic susceptibility.  
For one-dimensional, finite-range spin systems I've developed
exact, analytic methods for determining two computational measures of
pattern: the excess entropy and the epsilon-machine.  The
excess entropy is an information theoretic measure of the apparent
memory stored in a spatially-extended configuration.  The
epsilon-machine is a minimal stochastic model capable of
statistically reproducing a given system.  The epsilon-machine is
selected from the least computationally powerful model class(es) in a
stochastic generalization of the Chomsky hierarchy that admits
a finite description of the system.  Since the machine is minimal, it
may be viewed as the ``irreducible representation'' of the
approximate symmetries present in the system.
<p>

My results for finite-range spin systems have helped demonstrate that
this computational approach provides a rigorous way to discover and
quantify patterns and organization.  Moreover, I've found that my
techniques are clearly different from and complementary to those found
in statistical physics.  Specifically, the excess entropy serves as a 
wavenumber-independent detector of periodic behavior.  And I've found
that the epsilon-machine is essential for discovering structure in
highly entropic systems that are not well-approximated by a linear
combination of periodic components.  
This work is the first application of this computational approach to a
statistical mechanical system; previous work had been on deterministic
maps and deterministic cellular automata.  
These results have been published as <a href="Publications/SCS1DSS.html">
Statistical Complexity of Simple One-Dimensional Spin Systems</a>, in
<i>Physical Review E</i>.  A longer paper, ``Discovering Non-critical
Organization: Statistical Mechanical, Information Theoretic and
Computational Views of Patterns in One-Dimensional Spin Systems'' is
nearing completion and will be submitted to the <i>Journal of
Statistical Physics</i>. 
<p><br>


<h3> Future Work:</h3><p>

Having established that my computational approach is indeed different
from extant approaches to pattern in statistical mechanics, 
I now plan to apply my techniques to a wide
variety of equilibrium and non-equilibrium systems that are capable of
exhibiting substantially more complex structures than the finite-range
spin systems I've considered thus far.
A natural place to begin is the two-dimensional Ising model, the
``canonical'' example of a second-order phase transition.  This
project, a collaboration with Jim Crutchfield of the Santa Fe
Institute and Kristian Lindgren and Mats Nordahl of Goteborg
University, is already underway.  Most of the work will, by
necessity, be numerical.  However, I plan on using high and low
temperature series expansions to complement the simulations.  This
work will constitute the first thorough, fully two-dimensional
application of information and computation theory to a statistical
mechanical problem.  
<p>

Will these computational measures of pattern turn out to be universal
in the same way that, say, the susceptibility or the specific heat is
in the vicinity of the critical point?   These thermodynamic functions
are universal because the diverging length scale at the critical point
makes the details of short-range interactions irrelevant.  To support
these long range correlations, it is generally believed that the
system must possess an infinite amount of memory.  Hence, it is
expected that the excess entropy, a measure of the memory stored in
the lattice, will also diverge at the critical point.  Does the excess
entropy diverge in a manner that can be related to the divergence of
other statistical mechanical quantities?  More importantly, what
computation is needed to produce the long-range correlations:  how is
the (presumably infinite) memory organized?  Calculating the excess
entropy and the $\epsilon$-machines for the two-dimensional Ising
system will help us answer these questions and is an essential step
toward the understanding of how physical systems form patterns and
organize.
<p>

I also would like to work on non-equilibrium systems.  
Of particular interest are the ``self-organized criticality'' models that
have, somewhat controversially, been proposed as a universal mechanism
for a wide range of pattern-forming systems.  Proponents of
self-organized criticality suggest that it can describe the
distribution of earthquakes, traffic flow, flux lines in type-II
superconductors,  and the rates of extinction in
macro-evolution.  Does ``self-organized criticality'' really describe
the behavior of these disparate systems?  The computational approach 
that I use will help us gain a greater understanding of these models
and let us compare the pattern-forming abilities of different
systems in terms of how they process information.
<p>

Further down the road, after refining and extending my techniques by
studying the systems mentioned above, I'll be in a position to apply
my approach to a variety of more realistic models and even
experimental data.  Numerous imaging technologies---such as neutron
scattering, scanning tunneling and atomic force microscopes---are
improving quite 
rapidly.  It is now possible to gather quickly enormous amounts of
structural information about a material.  Likewise, larger and larger
amounts of information about biological systems are becoming
available, especially in neuroscience, molecular evolution and genetics. 
The task, then, will be to develop an automated way to find the
patterns or the ``interesting'' features in the sea of data.  The
computational approach to organization that I've worked with is
well-suited for this task.  
<p>

In summary, I am fascinated by the ways in which non-linear,
interacting systems can produce intricate patterns and
organized or collective behavior.  But at present, our notion of
pattern and organization is largely subjective.  Statistical mechanics
has a rather impoverished set of tools for discovering and quantifying
structure, pattern, information processing, and memory.  My research
thus far has show that computation and information theory
significantly enrich our set of tools for analyzing self-organizing
and pattern-forming systems.  In the years ahead, I look forward to
continuing to refine these tools and applying them to physical and
natural processes. 
<p>

<HR>

<p>
[<a href="index.html">Dave's Home Page</a>]

</html>




