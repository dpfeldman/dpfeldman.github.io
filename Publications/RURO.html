<html>
<head>
<title>Regularities Unseen, Randomness Observed: Levels of Entropy Convergence</title>
<h1>
<center>
Regularities Unseen, Randomness Observed: Levels of Entropy Convergence
</center>
</h1>
<p>
</head>
<body>
<br>
<p>
<h2>Citation</h2>
<blockquote>
J.P. Crutchfield and D. P. Feldman 
<cite>
Regularities Unseen, Randomness Observed: Levels of Entropy Convergence.
</cite> <i>Chaos</i>, 2003. <b>15</b>: 25-54.  2003.
   <a href="http://arXiv.org/abs/cond-mat/0102181">cond-mat
0102181</a>. 



</blockquote>
<h2>Abstract</h2>
<blockquote>


      We study how the Shannon entropy of sequences produced by an
      information source converges to the source's entropy rate. We  
      synthesize several phenomenological approaches to applying
      information theoretic measures of randomness and memory to 
      stochastic and deterministic processes by using successive
      derivatives of the Shannon entropy growth curve. This leads, in
      turn, to  
      natural measures of apparent memory stored in a source and the
      amounts of information that must be extracted from observations
      of 
      a source in order for it to be optimally predicted and for an
      observer to synchronize to it. One consequence of ignoring these 
      structural properties is that the missed regularities are
      converted to apparent randomness. We demonstrate that this
      problem arises 
      particularly for small data sets; e.g., in settings where one
      has access only to short measurement sequences.  
</blockquote>



<br>
<h3> Download the Paper:</h3>
<ul>
    <li> <a href="ruro.ps"> Postscript</a> (1.9Mb)
    <li> <a href="ruro.pdf"> PDF</a> (601kb)
    <li> <a href="ruro.ps.gz"> Gzipped Postscript</a> (524kb)
    <li> <a href="ruro.ps.Z"> Compressed Postscript</a> (793kb)
</ul>

</body>

<hr>
[<a href="../publications.html">Back to Publications Page</a>]


</html>
