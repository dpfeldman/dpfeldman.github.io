
<html>
<head>
<title> Statement of Research Interests </title>
</head>

<center>
<h2> Statement of Research Interests </h2>
<h3> David P. Feldman </h3>
</center>

My research is concerned with discovering and and quantifying
structure in a statistical setting:  either statistical mechanics or
the study of dynamical systems.  In particular, I seek answers to the
following questions:  How much apparent memory is stored in
spatially-extended configurations?  How can one discover and classify
the intrinsic computation ``embedded'' in a system?  That is, how much
memory is required to produce a configuration and <em> how</em> is that
memory utilized?<p>

Historically, many of these questions stem from the study of
dynamical systems in the 1970's and early '80's when it was realized
that the Shannon entropy provides a broadly-applicable tool that can
be used to quantify the unpredictability inherent in a system.
However, the entropy does not provide any measure of correlational
structure.  A good deal of effort, of which my work has been a small
part, has been expended over the past 15 years or so to try and fill
this void and define a general measure of global correlational
structure.  This broad area of research is sometimes referred to as
``complexity,'' a term I try to avoid, as I feel its widespread
overuse and misuse has largely stripped the word of its meaning. <p>


Questions of memory and measures of correlational structure also occur
naturally in a statistical mechanical setting.  For example, in an
interacting many-body system, we might wish to inquire about the
memory utilized by the system across space and/or time.  More
generally, we seek an answer to the question:  Is the system
organized, and if so, to what extent?  Statistical mechanics answers
these questions indirectly, at best.  
<p>


One approach to addressing theses questions
makes use of information theory
and quantifies a system's memory by means of the <em> excess entropy</em>: a
quantity which measures the manner in which entropy densities of
finite-size subsystems converge to the entropy density of the entire,
infinite system.  For translationally invariant systems, the excess
entropy is equivalent to the mutual information between two
semi-infinite halves of a configuration.  In the literature, excess
entropy goes by several other names, the most well-known of which is
the <em> Effective Measure Complexity</em>.
<p>

Another approach to these questions uses the architectural analysis of
information processing available within computation theory. 
Specifically, computational abilities of a system are inferred by first
determining the <em> causal states</em>,
which, roughly
speaking, are elements of the minimal set of aggregate variables
which can be used to statistically reproduce the original
configuration.  The causal state may thus be viewed as the ``true'' or
``hidden'' states of the process.  The causal states, together with
their transition probabilities, constitute a model---known as an
epsilon-machine---of the original process.  The model
is chosen to belong to the least powerful computational model
class(es) in a stochastic generalization of the Chomsky Hierarchy that
admits a finite description of the process.  The structure of the
epsilon-machine tells us how a system utilizes its memory to
produce the observed configurations. 
<p>

My work has mainly been concerned with calculating the above
quantities for model statistical mechanical systems.
The results of these calculations have enabled me to refine the
interpretations of the quantities and compare and contrast these newer
measures of structure and memory with extant quantities in statistical
mechanics and information theory.
<br>
<h2> Recent Accomplishments </h2>


<b> Exact Analytic Calculation of Excess Entropy,
and Determination of epsilon-machines for One-Dimensional,
Finite-Range Classical Spin Systems:</b><p>

Using transfer matrix techniques I developed exact analytic methods
to calculate the excess entropy and determine
the epsilon-machine for one-dimensional, finite range classical
spin systems.  These results enabled me to perform a
detailed comparison of statistical mechanical, information theoretical
and computational approaches to statistical structure and pattern.
There are two main conclusions that have emerged from   
this work.  First, I found that the excess entropy acts as a
wavenumber-independent ``order parameter'' for periodic structures.  In
contrast to the structure factors, which detect structure at a given
wavenumber, the excess entropy is sensitive to periodic order of any
periodicity.  

Second, this work has demonstrated that architectural measures of
structure provided by computation theory are particularly necessary
for the characterization of structure that is highly entropic and does
not have a strong periodic component.  The information theoretic and
statistical mechanical measures of structure fail to adequately
capture structure for these sorts of highly entropic, but not
structureless systems. 
<p>

<b> Analyzed and Critiqued a Recently-Proposed Measure of
Statistical Complexity:</b>

I have shown that a measure of statistical
complexity recently proposed by Lopez-Ruiz, Mancini, and Calbet
vanishes exponentially fast as a function of the system 
size for all finite-step ergodic Markov chains.  As a
result, the proposed complexity measure vanishes in the thermodynamic
limit for a broad class of highly-structured systems, making it of dubious
relevance in a statistical mechanical context.

<br>
<h2> Current Research</h2>

<b>
Extension of Results to more than One Dimension:</b>


I am currently working on
extending the definitions of excess entropy and causal states to more
than one dimension.  While fair amount has been written about how to do
this, almost no examples have been worked
through.  I am presently developing code to 
determine the causal states and $\epsilon$-machines
numerically.  Using my exact one-dimensional results,
I expect to develop a theory of 
``finite size scaling'' for $\epsilon$-machines.  I'll then consider
some reversible cellular automata and the two-dimensional
Ising model. <p>

<b>Multifractal Spectra:</b>

Calculating the multifractal spectrum is an often-used method to
characterize the range of fluctuations produced by a dynamical system.
Along the way, quantities analogous to the
partition function, temperature, and free energies or statistical
mechanics are encountered.  In my view, a clear interpretation of
these ``statistical mechanical-like'' quantities is still lacking.  I
am embarking on a line of work that I hope will solidify some of the
statistical mechanical analogies.  Specifically, I have calculated the
multifractal spectrum for one-dimensional spin systems.
I believe that
the simplicity of the model and the direct fashion in which the
thermodynamic formalism can be compared to thermodynamics itself will
help strengthen the interpretation of the multifractal spectrum and
related quantities.  A preliminary draft of a paper reporting this
work is underway. <p>

<HR>


[<a href="index.html">Dave's Home Page</a>]
<p>


</html>




