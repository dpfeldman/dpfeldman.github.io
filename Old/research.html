<header><title>David P. Feldman Research</title></header>

<body>
<center><h1>Research Interests</h1>
<h2><i> David P. Feldman </i></h2></center>


My research centers around the idea of complexity.  What is complexity?
Complex is the opposite of simple; the complex is that which is neither 
perfectly regular nor utterly random.   Thus, something which is complex 
has a long description.  At present, physics lacks a  
statistic which satisfactorily captures this notion of complexity.
This is a serious deficiency, for the vast majority of 
natural processes reside in the expanse between the random and
the perfectly regular.  <p>

Over the past year I've been looking at quantities that
could provide a measure of complexity or structure --
<i> Excess Entropy </I> and <I> Statistical Complexity</I>.  These
statistics have been put forth by Dr. James P. Crutchfield of the 
UC Berkeley Physics Department and the Santa Fe Institute.  My
work has been carried out under his guidance.<p> 

Briefly, a calculation of the statistical complexity involves examining a
data stream and formally building a computational model that can reproduce
that data in a statistical sense.  The statistical complexity is then a
measure of the informational size of the reconstructed model.  The
statistical complexity is set apart from other proposals for a complexity
measure because it explicitly makes reference to the computational class of 
the model being built.  In this sense, statistical complexity can be
viewed as a measure of the intrinsic computational ability of a system. <p>

Excess entropy, roughly speaking, is a measure of how the entropies of
successively larger subsets of a system converge to the entropy of the
entire system.  Excess entropy provides a rigorous lower bound for 
statistical complexity.  <p>


Recently, I have derived an exact method for determining the
excess entropy of any system described by a finite size transfer
matrix.  These include <i>all</i> one-dimensional Ising models with
finite range interaction, as well as one-dimensional Slater KDP
models.  The latter are noteworthy since they exhibit a first-order
phase transition between an ordered and a disordered state.<p>



Presently, excess entropy and statistical complexity have
been calculated for only a few systems, none of which have
a clear physical realization.  Filling this void by actually 
carrying out calculations of these quantities for physics systems
is the main thrust of my work.  There are two main goals. 
First, by calculating these quantities for well-understood systems, we will
learn more about how these measures of complexity behave.
Eventually, we hope to be able to make some general statements about the
origins of complexity.  Second, this approach is a new way of looking at
physical systems, and thus will extend our knowledge of the systems under
study.<p>


Thus far, my results confirm the anticipated behavior of the excess 
entropy; it is found to reach a maximum in regions in between perfect order
(zero entropy per site at absolute zero) and perfect disorder 
(corresponding to infinite temperature).  To carry out these calculations, I
have adapted the method of transfer matrices, a standard tool in
statistical mechanics.
I anticipate developing Monte Carlo simulations to complement my
analytic work.  For the time being, however, I would like to push 
the analytic work as far as I can.<p>


<h2>Future Plans</h2>

In general, I plan to continue examining the relationships between
computational ability -- measured via excess entropy and statistical
complexity -- and disorder.  Is it always the case that excess
entropy reaches a single maximum in between zero and infinite temperature?



My future research plans include examining the following: <ul>

<li> <b>Excess Entropy of 1D Classical Spin Systems.</b>  
In the very near future, I will complete a 
thorough treatment of the 1D Ising and Slater KDP models.
I also plan to consider continuous systems, such as the 1D XY model.
The goal is to gain a sense of the range of behaviors
exhibited by excess entropy.   
Plots of excess entropy vs. entropy density look qualitatively similar
over a wide range of models and parameter values.  Is there any 
quantitative relationship (scaling, universality) between these different 
behaviors? <p>

<li> <b>Excess Entropy of 1D Quantum Systems </b>.  Will quantum
systems exhibit different behavior than classical systems?  <p>

<li> <b>Excess Entropy of Disordered 1D Systems </b>. How does the
excess entropy behave for disordered systems?  Will this approach of
looking at the computational ability of systems lead to any new insight
into the physics involved?  <p>


<li> <b>Generalization to More than One Dimension</b>.  Once I enter
the world of two dimensions, I expect the range of behavior to become
much richer.  In particular, it will be possible to examine systems
that exhibit continuous (second-order) phase transitions.
The obvious system to examine first is the 2D nearest
neighbor Ising model in zero external field.  There are a wealth of
analytic tools available; I plan to use these tools to calculate --
hopefully analytically -- the excess entropy and statistical complexity.
<p>

</ul>

In the long term,
once a method for measuring complexity is refined and well-understood,
I would like to examine the origins of complexity and apply these
tools to other fields.  Particularly fruitful applications might be found
in the field of Evolutionary Genetics.  
At present, evolutionary theory has little to say
about the emergence of increasingly complex life forms.  Once a satisfactory
measure of complexity is put forth, we will at least have the 
mathematical vocabulary to begin to address these questions of the origins
of complexity in the natural and physical world.
<p>

<hr>
[<a href="home.html">Dave's Home Page</a>]
<p>

</body>
