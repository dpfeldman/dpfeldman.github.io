<html>
<head>
<title> Abstract for Dave Feldman's Talk: </title>
</head>

<Center>

   <p>
<p>
<h1> <I> Complexity:</I>  What is it and can it be measured? </h1>

</Center>

<HR>
<p>


What is complexity?  In seems fair to say that complex is the
opposite of simple.  Things that are difficult to describe are
complex.  The utterly random --- a coin toss or a heat bath ---
and the completely periodic -- a perfect crystal -- are easy to
describe and are thus not complex.  Complexity lives between order
and chaos.  
<p>

Thermodynamic entropy and temperature  are well-suited
to analyze randomness.  And the language of group theory provides
an elegant description of the symmetries and regularites present in
a process.  But at present, physics is relavtively mute on the
issue of complexity. This is a serious deficiency, for the vast 
majority of natural processes reside in the  expanse between the 
random and the perfectly regular.   <p>

In my talk, I will review an approach for quantifying complexity.
This approach, known as <A HREF="http://www.santafe.edu/projects/CompMech/" > 
Computational Mechanics </A>
and developed by
 <A HREF="http://www.santafe.edu:80/~jpc/" > Jim Crutchfield </A>, 
is an amalgam of ideas from dynamical systems, information theory, 
and formal computation theory.

I will begin by briefly giving some motivation for why we are trying 
to measure complexity.  I will put the question in
context and state clearly what it is we seek in a complexity measure. 
<p>

I will then introduce some relatively standard notions of entropy and
uncertainty contained in information theory.  After a quick discussion of
formal computational models, I will introduce two statistics designed 
to measure complexity:  excess entropy and statistical complexity.
I shall conclude by presenting the results of some calculations of these
quantities I carried out using the 1D Ising model.  
<p>



</html>